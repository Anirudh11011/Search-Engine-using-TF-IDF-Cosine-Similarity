# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b2MjXEwy4kZdpZY7DgUgQjCL7hq_Uvw2
"""

import os
import math
import nltk
nltk.download('stopwords')
from collections import defaultdict
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

corpusroot = './US_Inaugural_Addresses'
pre_processed_data = []

tokenizer = RegexpTokenizer(r'[a-zA-Z]+')
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

doc_names = []

for filename in os.listdir(corpusroot):
    if filename.endswith('.txt'):
        doc_names.append(filename)
        file_path = os.path.join(corpusroot, filename)
        with open(file_path, "r", encoding='windows-1252') as file:
            doc = file.read().lower()

        # Done all preprocessing for the documents like Tokenization, stop word removal, and stemming.
        tokens = tokenizer.tokenize(doc)
        filtered_tokens = [w for w in tokens if w not in stop_words]
        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]
        pre_processed_data.append(stemmed_tokens)

N = len(pre_processed_data)

tf = []
df = defaultdict(int)


#Calculated term Frequency and document frequency
for doc in pre_processed_data:
    tf_dict = defaultdict(int)
    for token in doc:
        tf_dict[token] += 1
    tf.append(tf_dict)
    for token in set(doc):
        df[token] += 1

tfidf_vectors = []
postings = defaultdict(list)

#here tf-idf weights has been calculated
for doc_id, doc_tf in enumerate(tf):
    tfidf = {}
    vector_length = 0
    for token, tf_value in doc_tf.items():
        tf_weight = 1 + math.log10(tf_value)
        idf_weight = math.log10(N / df[token])
        tfidf[token] = tf_weight * idf_weight
        vector_length += tfidf[token] ** 2
    vector_length = math.sqrt(vector_length)
    if vector_length > 0:
        for token in tfidf:
            tfidf[token] /= vector_length
            postings[token].append((doc_id, tfidf[token]))
    tfidf_vectors.append(tfidf)

for token in postings:
    postings[token].sort(key=lambda x: x[1], reverse=True)

def getidf(token):
    token_lower = token.lower()
    stemmed_token = stemmer.stem(token_lower)
    if stemmed_token in df:
        idf_value = math.log10(N / df[stemmed_token])
        return idf_value
    else:
        return -1

def getweight(doc_name, token):
    token_lower = token.lower()
    stemmed_token = stemmer.stem(token_lower)
    if doc_name in doc_names:
        doc_index = doc_names.index(doc_name)
        tfidf_vector = tfidf_vectors[doc_index]
        if stemmed_token in tfidf_vector:
            token_weight = tfidf_vector[stemmed_token]
        else:
            token_weight = 0.0
        return token_weight
    else:
        return 0.0

#function to check whether to go deeper and search for more records or not
#preprocessing is done again for queries
def query_with_limit(sentence, limit):

    sentence = sentence.lower()
    tokens = tokenizer.tokenize(sentence)
    filtered_tokens = [w for w in tokens if w not in stop_words]
    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]

    qtf = defaultdict(int)
    for token in stemmed_tokens:
        qtf[token] += 1

    qtfidf = {}
    vector_length = 0
    for token, tf_value in qtf.items():
        tf_weight = 1 + math.log10(tf_value)
        idf_weight = getidf(token)
        qtfidf[token] = tf_weight * idf_weight
        vector_length += qtfidf[token] ** 2
    vector_length = math.sqrt(vector_length)
    if vector_length > 0:
        for token in qtfidf:
            qtfidf[token] /= vector_length

    token_top_limit = {}
    term_upper_bound = {}
    for token in qtfidf:
        if token in postings:
            postings_list = postings[token]
            token_top_limit[token] = postings_list[:limit]
            if len(postings_list) >= limit:
                term_upper_bound[token] = postings_list[limit - 1][1]
            else:
                term_upper_bound[token] = postings_list[-1][1]
        else:
            token_top_limit[token] = []
            term_upper_bound[token] = 0.0


    shortlisted_doc_ids = set()
    for token in token_top_limit:
        for posting in token_top_limit[token]:
            shortlisted_doc_ids.add(posting[0])

    if not shortlisted_doc_ids:
        return ("None", 0.0)


    shortlisted_actual_score = {}
    shortlisted_upper_score = {}
    for doc_id in shortlisted_doc_ids:
        actual_score = 0.0
        upper_score = 0.0
        for token in qtfidf:
            q_weight = qtfidf[token]
            term_found = False
            found_term_weight = 0.0
            for posting in token_top_limit[token]:
                if posting[0] == doc_id:
                    term_found = True
                    found_term_weight = posting[1]
                    break
            if term_found:
                actual_score += q_weight * found_term_weight
                upper_score += q_weight * found_term_weight
            else:
                upper_score += q_weight * term_upper_bound[token]
        shortlisted_actual_score[doc_id] = actual_score
        shortlisted_upper_score[doc_id] = upper_score


    overall_upper_bound = 0.0
    for token in qtfidf:
        overall_upper_bound += qtfidf[token] * term_upper_bound[token]

    best_doc = None
    best_doc_score = -1.0
    for doc_id in shortlisted_doc_ids:
        if shortlisted_actual_score[doc_id] > best_doc_score:
            best_doc_score = shortlisted_actual_score[doc_id]
            best_doc = doc_id


    is_valid = True
    for doc_id in shortlisted_doc_ids:
        if doc_id != best_doc:
            if best_doc_score < shortlisted_upper_score[doc_id]:
                is_valid = False
                break
    if best_doc_score < overall_upper_bound:
        is_valid = False


    if is_valid:
        return (doc_names[best_doc], best_doc_score)
    else:
        return ("fetch more", best_doc_score)

#query function will call query_with_limit to check whether to go deeper or not and here I have set intial limit of documents to search is 10
#  and it keep on increasing by 10 if it wants to go deeper
def query(sentence, initial_limit=10, step=10, max_limit=100):
    limit = initial_limit
    result = ("fetch more", 0.0)
    while limit <= max_limit:
        result = query_with_limit(sentence, limit)
        if result[0] != "fetch more":
            return result
        print(f"Fetching more documents... Increasing search limit to {limit+step}")
        limit += step
    return result

if __name__ == "__main__":
    print("%.12f" % getidf('british'))
    print("%.12f" % getidf('union'))
    print("%.12f" % getidf('dollar'))
    print("%.12f" % getidf('constitution'))
    print("%.12f" % getidf('power'))
    print("--------------")
    print("%.12f" % getweight('19_lincoln_1861.txt','states'))
    print("%.12f" % getweight('07_madison_1813.txt','war'))
    print("%.12f" % getweight('05_jefferson_1805.txt','false'))
    print("%.12f" % getweight('22_grant_1873.txt','proposition'))
    print("%.12f" % getweight('16_taylor_1849.txt','Abraham Lincoln '))
    print("--------------")
    print("(%s, %.12f)" % query("executive power"))
    print("(%s, %.12f)" % query("foreign government"))
    print("(%s, %.12f)" % query("public rights"))
    print("(%s, %.12f)" % query("people government"))
    print("(%s, %.12f)" % query("states laws"))
    # my tesed queries
    print("(%s, %.12f)" % query("and aspirations of those illustrious benefactors of their age and nation. It has promoted the lasting welfare of that country so dear to us all; it has to an extent far beyond the ordinary lot of humanity secured the freedom and happiness of this people. We now receive it as a precious inheritance from those to whom we are indebted for its establishment, doubly bound by the examples which they have left us and by the blessings which we have enjoyed as the fruits"))
    print("(%s, %.12f)" % query("only because slavery "))
    print("(%s, %.12f)" % query("Abraham Lincoln "))
    print("(%s, %.12f)" % query("Mustang"))
    print("%.12f" % getweight('16_taylor_1849.txt','Abraham Lincoln '))
    print("%.12f" % getidf('UTA'))